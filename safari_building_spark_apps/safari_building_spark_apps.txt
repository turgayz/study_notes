java install:
You can use WebUpd8 PPA (this will download the required files from Oracle and install JDK 8):

sudo apt-add-repository ppa:webupd8team/java
sudo apt-get update
sudo apt-get install oracle-java8-installer
Are PPA's safe to add to my system and what are some “red flags” to watch out for?

Also ensure your JAVA_HOME variable has been set to:

/usr/lib/jvm/java-8-oracle
For this you can use the following command (see step 3 of Manual Install to see how to make it permanent):

export JAVA_HOME=/usr/lib/jvm/java-8-oracle
----------------
scala install:
export SCALA_HOME=~/programs/scala-2.11.7
export PATH=$PATH:$SCALA_HOME/bin
----------------
SPARK install:
export SPARK_HOME=/home/turg/SPARK/spark-1.6.0-bin-hadoop2.6
export PATH=$PATH:$SPARK_HOME/bin
----------------
anaconda install:
/home/turg/programs/anaconda2
----------------
hadoop install
/home/turg/programs/hadoop-2.6.4/
export HADOOP_HOME=/home/turg/programs/hadoop-2.6.4
export PATH=$PATH:$HADOOP_HOME/bin

hadoop_env.sh: export JAVA_HOME=/usr/lib/jvm/java-8-oracle
spark_env.sh: export HADOOP_CONF_DIR=/home/turg/programs/hadoop-2.6.4/etc/hadoop/
----------------
change hostname
Edit /etc/hostname , make the name change, save the file.
You should also make the same changes in /etc/hosts file.
- in master, delete "127.0.0.1 master" line from /etc/hosts
----------------
ssh setup:
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
ssh-copy-id -i ~/.ssh/id_dsa.pub turg@master
ssh-copy-id -i ~/.ssh/id_dsa.pub turg@slave1
ssh-copy-id -i ~/.ssh/id_dsa.pub turg@slave2

scp spark-env.sh slave1:/home/turg/SPARK/spark-1.6.0-bin-hadoop2.6/conf/
scp spark-env.sh master:/home/turg/SPARK/spark-1.6.0-bin-hadoop2.6/conf/
----------------
access from spark:
export SPARK_LOCAL_IP=laptop
spark-shell --master=spark://master:7077

bin/hdfs dfs -fs hdfs://master:9000 -mkdir /sil
bin/hdfs dfs -fs hdfs://master:9000 -put /home/turg/SPARK/advanced_analytics/profiledata_06-May-2005/user_artist_data.txt /sil
val rawUserArtistData = sc.textFile("hdfs://master:9000/sil/user_artist_data.txt")

----------------

Driver: Process that contains the SparkContext
Worker: Node that can run application code

RDD: Resilient: if the data in memory (or on a node) is lost, it can be recreated
     Distributed: data is chunked into partitions and stored in memory across the cluster 
     Dataset: initial data can come from a file or be created programmatically

RDDs are read-only and immutable

Transformations: Lazy Evaluation, Returns new RDD
Actions: Materialize data (Evaluates RDD lineage), Returns final value to driver

flatMap: maps each element to an iterator, and collects all values from all iterators:
nums = sc.parallellize([1,2,3])
nums.flatMap(lambda x: range(x)) --> {0,0,1,0,1,2}

nums.reduce(lambda x,y: x+y) --> 6

nums.saveAsTextFile("nums.txt")   

Key-Value operations:

pets = sc.parallelize([("cat", 1), ("dog", 1), ("cat", 2)])

pets.reduceByKey(lambda x,y: x+y).collect() --> [('dog', 1), ('cat', 3)]
(first groups, then reduces)

pets.groupByKey().collect() --> [("dog", iterable), ("cat", iterable)]

SQLContext, DataFrame

ipython notebook ile sqlcontext, csv için, başlatmadan önce:
export PACKAGES="com.databricks:spark-csv_2.11:1.3.0"
export PYSPARK_SUBMIT_ARGS="--packages ${PACKAGES} pyspark-shell"

import pyspark as ps
sc = ps.SparkContext()

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
link = "/home/turg/SPARK/livelessons/spark-live-lessons-data/bulk_data/airline-data/"

df = sqlContext.read.format("com.databricks.spark.csv")\
.option("header", "true")\
.option("inferSchema", "true")\
.load(link)

df.head()
df.schema 

subset = df.select("DEP_DELAY", "ARR_DELAY", "ORIGIN_AIRPORT_ID", "DEST_AIRPORT_ID")
subset.show()

subset.dtypes

link_meals = "/home/turg/SPARK/livelessons/spark-live-lessons-data/bulk_data/readychef/meals.txt"
link_events = "/home/turg/SPARK/livelessons/spark-live-lessons-data/bulk_data/readychef/events.txt"

meals_rdd = sc.textFile(link_meals)
events_rdd = sc.textFile(link_events)

header_meals = meals_rdd.first()
header_events = events_rdd.first()

meals_no_header = meals_rdd.filter(lambda row: row != header_meals)
events_no_header = events_rdd.filter(lambda row: row != header_events)

meals_json = meals_no_header.map(lambda row: row.split(';')).map(lambda row_list: dict(zip(header_meals.split(';'), row_list)))
events_json = events_no_header.map(lambda row: row.split(';')).map(lambda row_list: dict(zip(header_events.split(';'), row_list)))

meals_df = sqlContext.jsonRDD(meals_json)
events_df = sqlContext.jsonRDD(events_json)

def type_conversion(d, columns):
   for c in columns:
       d[c] = int (d[c])

   return d

import json   
meals_typed = meals_json.map(lambda j: json.dumps(type_conversion(j, ['meal_id', 'price'])))
events_typed = events_json.map(lambda j: json.dumps(type_conversion(j, ['meal_id', 'userid'])))
meals_df = sqlContext.jsonRDD(meals_json)
events_df = sqlContext.jsonRDD(events_json)

meals_df.registerTempTable('meals')
events_df.registerTempTable('events')
sqlContext.sql('select * from meals limit 5').collect()
   
sqlContext.sql("""
    SELECT type, COUNT(type) as cnt FROM meals
    INNER JOIN events on meals.meal_id = events.meal_id
    WHERE events.event = 'bought'
    GROUP BY meals.type
    ORDER BY cnt DESC
""").collect()

To debug, use local sc:
sc = ps.SparkContext('local')
This way debug will be easy because sc will run in a single thread

column.isNull()
dataFrame.dropna(column_name)
dataFrame.fillna()
dataFrame.replace(to_erplace, value)
pyspark.accumulators

df.filter(df_dates['students_reached'].isNull()).select('students_reached', 'funding_status').collect()

df_no_null = df.fillna(0, ['students_reached'])

freq_items = df.freqItems(['school_city', 'grade_level'], 0.7).collect()

df.select(....).describe().show()

rdd.histogram()
rdd.stats()
df.groupby('column_name').count()
df.describe('column_name')

df.crosstab()
df.corr()
